[[0]] 
Efficient large data operations with Biggus
===========================================
Aug 30th 2014

+++ADD+++ www.github.com/SciTools/biggus

Patrick Peglar - UK Met Office, Exeter

::  Intro self; Met Office; AVD
::    science support role, especially tools development
::    Main product = Iris
::      "Python library for analysing and visualising meteorological and oceanographic data sets."
::         - this is now in use around the world, but PRINCIPALLY by a large in-house science base (~400)
::         - key points
::             * fully open-source
::             * Python, numpy, matplotlib
::             * file-format agnostic
::             * processing and visualisation
::    major sub projects :  Cartopy (matplotlib mapping), Biggus (deferred data operations)

+++
>>>
Image (Office logo)
+++


[[0a]] Overview
-------
Biggus is a lightweight pure-Python package which implements lazy operations on numpy array-like objects. This provides dramatically improved efficiency in analysing large datasets, for minimal additional effort in the client code.

>>>
From the module docstring...
------
Virtual arrays of arbitrary size, with arithmetic and statistical operations, and conversion to NumPy ndarrays.
Virtual arrays can be stacked to increase their dimensionality, or tiled to increase their extent.
Includes support for easily wrapping data sources which produce NumPy ndarray objects via slicing.
** For example: netcdf4python Variable instances, and NumPy ndarray instances.
All operations are performed in a lazy fashion to avoid overloading system resources.
Conversion to a concrete NumPy ndarray requires an explicit method call.

::  In a nutshell
::  Group, subset, slice+dice, arithmetic, statistical measures
::    -- virtual arrays : deferred calculations + data loading, storage-agnostic
::  
::  (ultimately : distribution agnostic ?)

>>>
( That's pretty much it, so we can all go home. )


[[1]] The longest journey ...
------
>>>
In [2]:
import biggus
print biggus.__version__
0.7.0-alpha


[[2]]  A simple 'mean' calculation ...
-----
(and later.. the biggus equivalent)

>>>
In [3]:
    import numpy as np
    array_1 = np.array([[1., 5., 2.], [7., 6., 5.]])
    print 'simple array :'
    print array_1
    print
    print 'shape :', array_1.shape
    mean_a1 = array_1.mean(axis=1)
    print
    print 'mean over axis 1 :'
    print mean_a1
simple array :
[[ 1.  5.  2.]
 [ 7.  6.  5.]]

shape : (2, 3)

mean over axis 1 :
[ 2.66666667  6.        ]


[[3a]]  simple mean calculation : The Biggus equivalent
-------

>>>
In [4]:
    lazy_1 = biggus.NumpyArrayAdapter(array_1)
    print 'a lazy array : ', lazy_1

a lazy array :  <NumpyArrayAdapter shape=(2, 3) dtype=dtype('float64')>


[[3b]]
Lazy mean and result...

>>>
In [5]:
    lazy_mean = biggus.mean(lazy_1, axis=1)
    print 'lazy mean :', lazy_mean
    print 'shape :', array_1.shape
    print
    print 'lazy mean *result* :'
    print lazy_mean.ndarray()

lazy mean : <_Aggregation shape=(2,) dtype=dtype('float64')>
shape : (2, 3)

lazy mean *result* :
[ 2.66666667  6.        ]

>>>
Same as before...
------
But this time change the source data, between forming the mean and evaluating it.

In [6]:
    lazy_mean = biggus.mean(lazy_1, axis=1)
    print lazy_mean
    array_1[0,:] = -1
    print lazy_mean.ndarray()

<_Aggregation shape=(2,) dtype=dtype('float64')>
[-1.  6.]


[[3]]
So... what is the point ?
------

>>>
 * CONVENIENCE (or.. "features")
    * grab all the data
    * write analysis code
    * don't worry about resources / efficiency

:: By far the *easiest* way to write a data processing algorithm starts by grabbing all the data
::   - typically as numpy arrays
::   - frequently load whole files, then cut out what you need (by type or region)
::
:: The key problem here is that as the data grows, the space+time cost gets unmanageable
::   - problems are soluable "DIVIDE + CONQUER"
::   - but these problems get mixed up with the original analysis solution
:: 
:: SO the primary purpose of Biggus is to separate data management from processing.
::

Example 
((Iris load output))

:: IN OUR HISTORY:
::  - file format independence
::  - basic deferred loading 
::  - history of 3 biggies
::     1. at one point separate code for PP + NC, nothing for GRIB
::     2. migrated PP to Biggus
::     3. shortly followed by NC --> GRIB now very easy !
:: see the benefit of abstraction


>>>
 * SPACE (resource efficiency)
    * data grows
    * eventually you can't fit all the data

:: (As already) DATA GROWS
::   - typically (IN OUR WORLD) = longer timescales or higher time/space resolution
:: 

:: In this case, even a simple time-mean : bigger data ==> won't all fit
::   - KEY SIMPLE CASE: the result fits, but not all the data does
::   - this problem is clearly fixable, but 
::   - with Biggus lazy evaluation, we can fix this


:: The biggest
::
>>>
 * TIME (efficiency + cleverness)
    * once you have divided loading, you don't want to re-scan
    * the processes that acces it must run in parallel
    * eventially / potentially this leads to distributed processing
        - (i.e. processes not co-hosted)



[[4]] Features summary
----
 1. arrays and indexing
 2. grouped arrays (aka: stack + tile)
 3. statistics  (aka: aggregation)
 4. arithmetic
 5. parallelism


+++
[[5]] Features #1 : **Arrays and Indexing**
### Anything that 'looks like' an array can be presented as a biggus.Array
-------
So, "What is an array ?"

It just needs to support the minimum numpy-array-like properties :

 * shape
 * dtype
 * __getitem__ (i.e. indexing)

+++
>>>
[[5a]]
### All types of Biggus array then support the major indexing operations...
[[Picture]]

:: Not quite as trivial as it looks
:: In numpy there are about +++?? 9 ? different types of array indexing
:: Biggus supports a useful subset...


>>>
+++
[[5b]]
Here's a tiny example that "looks like" an array full of a constant, and indicates clearly when it is actually accessed.
----

In [7]:
    class constant_array(object):
        def __init__(self, shape, value=0.0):
            self.shape = shape
            self.dtype = np.float
            self._value = value
        def __getitem__(self, indices):
            print 'accessing :', indices
            return  self._value * np.ones(self.shape)[indices]

    lazy_const_2x3 = biggus.NumpyArrayAdapter(constant_array((2, 3, 4), value=3.5))
    print lazy_const_2x3
    one_element = lazy_const_2x3[0, 1]
    print one_element
    print one_element.ndarray()

<NumpyArrayAdapter shape=(2, 3, 4) dtype=<type 'float'>>
<NumpyArrayAdapter shape=(4,) dtype=<type 'float'>>
accessing : (0, 1)
[ 3.5  3.5  3.5  3.5]


+++
[[5c]]
In fact, this functionality is already provided in biggus
-----
(without the evaluation debug).

It is called a ConstantArray..

In [8]:
    lazy_const_2x3 = biggus.ConstantArray((2, 3, 4), 3.77)
    print lazy_const_2x3
    one_element = lazy_const_2x3[0, 1]
    print one_element
    print one_element.ndarray()

<ConstantArray shape=(2, 3, 4) dtype=dtype('float64')>
<ConstantArray shape=(4,) dtype=dtype('float64')>
[ 3.77  3.77  3.77  3.77]

[[8]]



[[6]] Features #2 : **Grouped Arrays**
### Arrays can be combined into larger arrays
Two methods:
 1. "stack" arranges a set of identical shapes into a new dimension
 2. "mosaic" joins arrays edge-to-edge

>>>
[6a]
ArrayStack
----
Analogous to numpy "stack" operations.
A new dimension is always created
((code example ==to match image))
((Image))

>>>
[6b]
LinearMosaic
----
Analogous to numpy "concatenate" operations.
 * does *not* create a new dimension
((code example ==to match image))
((Image))

Note:
 * this handles irregular sizes
 * the sources can be arranged in a multidimensional shape
    :: prepended to the individual shape

:: (IN OUR WORLD) useful for calendar operations
:: note how powerful this is for regional extraction


[[7]] Features #3 : **Statistics**
We've already seen how to calulate simple statistics (the mean).
Others follow the same pattern
:: Though we currently support only the basics = 

Implementation:
  * each operation can be applied to subsections of the data, in a series of steps
    * initialise : process "chunks" : finalise

:: Clearly requires independent coding for each operation
:: hence the lack of generality ==> explicit operations, own naming

[[8]] Features #4 : **Arithmetic**
So-called "Elementwise" operations are provided.
Somewhat analagous to numpy basic functions (UFUNCS)
   ??? IS THIS APPROPRIATE ??
For example:
((code example))

:: at present, just add and sub
:: the chunk-aware implementation (a pointwise function) seems obvious
:: unlike statistics, this is not very deep, and **can** be generalised


[[9]] Features #5 : **Parallelism and Concurrency**
((Image))
