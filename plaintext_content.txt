[[0]] 
Efficient large data operations with Biggus
===========================================
Aug 30th 2014

Patrick Peglar - UK Met Office, Exeter

Biggus: https://github.com/SciTools/biggus

( this talk: https://github.com/pp-mo/nb-slideshow-template )


:: hi there, I'm Patrick, and I work at the UK Met Office.
:: I'm a member of the AVD team 
::    -- standing for "Analysis, Data and Visualization"
:: we develop and support software tools for scientific data analysis and presentation
:: nearly everything we produce is now open-source
::    -- on the basis that what is good for everyone is good for us
::
:: I'm here to talk about Biggus, our lightweight lazy array package
:: However, our main project is actually "Iris" :
::   -- "a Python library for analysing and visualising meteorological and oceanographic data sets."
::         - this is now in use around the world, but PRINCIPALLY by a large in-house science base (~400)
::         - key points
::             * processing and visualisation
::             * based on Python, numpy/scipy and matplotlib
::             * storage-format agnostic
::             * fully open-source
::    major sub projects :  Cartopy (matplotlib mapping), Biggus (deferred data operations)
::
:: Hoping for:
???
::    -- show "ordinary" users 

[[0a]] Overview
-------
Biggus is a lightweight pure-Python package which implements lazy operations on numpy array-like objects. This provides dramatically improved efficiency in analysing large datasets, for minimal additional effort in the client code.

>>>

+++KILL ?? +++
From the module docstring...
------
Virtual arrays of arbitrary size, with arithmetic and statistical operations, and conversion to NumPy ndarrays.
Virtual arrays can be stacked to increase their dimensionality, or tiled to increase their extent.
Includes support for easily wrapping data sources which produce NumPy ndarray objects via slicing.
** For example: netcdf4python Variable instances, and NumPy ndarray instances.
All operations are performed in a lazy fashion to avoid overloading system resources.
Conversion to a concrete NumPy ndarray requires an explicit method call.

+++
Key concepts: 
 * interchangeable terms : 'virtual array'; 'Lazy'  or 'deferred' evalution
 * a virtual array does not load actual values of the data ...
 * ... but it knows all about it:
   * size and shape
   * datatype
   * how to load (any given part of) it

:: ~EQUIVALENT TERMS: virtual, lazy, deferred, 
::
:: THE KEY MESSAGE: lazy evaluation
::   really not a complex idea -- and Biggus is quite a minimal implementation
::
:: An ordinary array (like numpy) *CONTAINS* the data concerned
::   -- to make one, you need to load all the data
::   -- this imposes a size limit (eventually)
::        -- you run out of space + start thrashing the disk
::
:: A **virtual** (Biggus) array does not care about the data values
::   -- it knows the shape and size and type
::   -- it knows where to get the data, and will give you any portion on request 
::   -- there is no practical size limit



+++
>>>
What Biggus provides:
 * virtual arrays of arbitrary size
 * combine and extract
 * statistics
 * arithmetic
 * efficent evaluation

Character of Biggus:
 * small, lightweight
 * usage is explicit
 * work-in-progress
 
:: [SMALL+LIGHT]
::    -- currently in a single module
::    -- easily understood (an afternoon), but somewhat minimal ((agile))
:: [EXPLICIT]
::    -- it doesn't attempt to simulate other APIs (except maybe indexing)
::    -- it doesn't do much "automatically" (no unexpected operations, no hidden cleverness)
:: [WIP] : plenty of scope to add useful things, plenty of things missing


>>>
+++DITCH??+++
( That's pretty much it, so we can all go home. )
+++
What's to come:
 * dead-simple example
 * what it's good for
 * summary of current features
 * hints about future directions


[[1]] The longest journey ...
------
+++
Start with very simple code examples.
>>>
In [2]:
import biggus
print biggus.__version__
0.7.0-alpha


[[2]]  A simple 'mean' calculation ...
-----
(and later.. the biggus equivalent)

>>>
In [3]:
    import numpy as np
    array_1 = np.array([[1., 5., 2.], [7., 6., 5.]])
    print 'simple array :'
    print array_1
    print
    print 'shape :', array_1.shape
    mean_a1 = array_1.mean(axis=1)
    print
    print 'mean over axis 1 :'
    print mean_a1
simple array :
[[ 1.  5.  2.]
 [ 7.  6.  5.]]

shape : (2, 3)

mean over axis 1 :
[ 2.66666667  6.        ]


[[3a]]  simple mean calculation : The Biggus equivalent
-------

>>>
In [4]:
    lazy_1 = biggus.NumpyArrayAdapter(array_1)
    print 'a lazy array : ', lazy_1

a lazy array :  <NumpyArrayAdapter shape=(2, 3) dtype=dtype('float64')>


[[3b]]
Lazy mean and result...

>>>
In [5]:
    lazy_mean = biggus.mean(lazy_1, axis=1)
    print 'lazy mean :', lazy_mean
    print 'shape :', array_1.shape
    print
    print 'lazy mean *result* :'
    print lazy_mean.ndarray()

lazy mean : <_Aggregation shape=(2,) dtype=dtype('float64')>
shape : (2, 3)

lazy mean *result* :
[ 2.66666667  6.        ]

>>>
Same as before...
------
But this time change the source data, between forming the mean and evaluating it.

In [6]:
    lazy_mean = biggus.mean(lazy_1, axis=1)
    print lazy_mean
    array_1[0,:] = -1
    print lazy_mean.ndarray()

<_Aggregation shape=(2,) dtype=dtype('float64')>
[-1.  6.]


[[3]]
So... what is the point ?
------

>>>
 * CONVENIENCE (or.. "features")
    * grab all the data
    * write analysis code
    * don't worry about resources / efficiency

:: By far the *easiest* way to write a data processing algorithm starts by grabbing all the data
::   - typically as numpy arrays
::   - frequently load whole files, then cut out what you need (by type or region)
::
:: The key problem here is that as the data grows, the space+time cost gets unmanageable
::   - problems are solvable "DIVIDE + CONQUER"
::   - but these problems get mixed up with the original analysis solution
:: 
:: SO the primary purpose of Biggus is to separate data management from processing.
::


:: CONVENIENCE is *also* EFFICIENCY 
::    -- your time
::    -- manageable code

+++
Example 
((Iris load output))

:: IN OUR HISTORY:
::  - file format independence
::  - basic deferred loading 
::  - history of 3 biggies
::     1. at one point separate code for PP + NC, nothing for GRIB
::     2. migrated PP to Biggus
::     3. shortly followed by NC --> GRIB now very easy !
:: see the benefit of abstraction


>>>
 * SPACE (resource efficiency)
    * data grows
    * eventually you can't fit all the data

:: (As already) DATA GROWS
::   - typically (IN OUR WORLD) = longer timescales or higher time/space resolution
:: 

:: In this case, even a simple time-mean : bigger data ==> won't all fit
::   - KEY SIMPLE CASE: the result fits, but not all the data does
::   - this problem is clearly fixable, but 
::   - with Biggus lazy evaluation, we can fix this
::      ... *** as we shall see... ***


>>>
 * TIME (efficiency + cleverness)
    * with sectional access (especially of files), you don't want to re-scan
    * the accessing processes must run "in parallel"
    * eventially / potentially this leads to distributed processing
        - (i.e. processes not co-hosted)



[[4]] Features summary
----
 1. arrays and indexing
 2. grouped arrays (aka: stack + tile)
 3. statistics  (aka: aggregation)
 4. arithmetic
 5. parallel evaluation

>>>
Key points:
 * all operations produce an 'Array' object
 * all operations can be combined arbitrarily
 * lazy representation allow efficient evaluation


[[5]] Features #1 : **Arrays and Indexing**
### Anything that 'looks like' an array can be presented as a biggus.Array
-------
So, "What is an array ?"

It just needs to support the minimum numpy-array-like properties :

 * shape
 * dtype
 * __getitem__ (i.e. indexing)

:: achieved via a thin interface layer

>>>
[[5a]]
### All types of Biggus array then support the major indexing operations...
[[Picture]]

:: Not quite as trivial as it looks
:: In numpy there are about +++?? 9 ? different types of array indexing
:: Biggus supports a useful subset...
:: N.B. we do "Ortho" indexing, not numpy "fancy" indexing
::    - this is like netcdf4 and h5py

>>>
[[5b]]
Here's a tiny example that "looks like" an array full of a constant, and indicates clearly when it is actually accessed.
----

+++CHANGED CODE+++
In [7]:
    class constant_array(object):
        def __init__(self, shape, value=0.0):
            self.shape = shape
            self.dtype = np.array(value).dtype
            self._value = value
        def __getitem__(self, indices):
            print 'accessing :', indices
            return  self._value * np.ones(self.shape)[indices]

    lazy_const_2x3 = biggus.NumpyArrayAdapter(constant_array((2, 3, 4), value=3.5))
    print lazy_const_2x3
    one_element = lazy_const_2x3[0, 1]
    print one_element
    print one_element.ndarray()

<NumpyArrayAdapter shape=(2, 3, 4) dtype=<type 'float'>>
<NumpyArrayAdapter shape=(4,) dtype=<type 'float'>>
accessing : (0, 1)
[ 3.5  3.5  3.5  3.5]


[[5c]]
In fact, this functionality is already provided in biggus
-----
(without the evaluation debug).

It is called a ConstantArray..

In [8]:
    lazy_const_2x3 = biggus.ConstantArray((2, 3, 4), 3.77)
    print lazy_const_2x3
    one_element = lazy_const_2x3[0, 1]
    print one_element
    print one_element.ndarray()

<ConstantArray shape=(2, 3, 4) dtype=dtype('float64')>
<ConstantArray shape=(4,) dtype=dtype('float64')>
[ 3.77  3.77  3.77  3.77]

:: NOTE just the same as the previous


[[6]] Features #2 : **Grouped Arrays**
### Arrays can be combined into larger arrays
Two methods:
 1. "stack" arranges a set of identical shapes into a new dimension
 2. "mosaic" joins arrays edge-to-edge

>>>
[6a]
ArrayStack
----
Analogous to numpy "stack" operations.
A new dimension is always created
((code example ==to match image))
((Image))

+++
?? cover the multidim interface
:: note the shape goes at the front

>>>
[6b]
LinearMosaic
----
Analogous to numpy "concatenate" operations.
 * does *not* create a new dimension
((code example ==to match image))
((Image))

Note:
 * this handles irregular sizes
 * the sources can be arranged in a multidimensional shape
    :: prepended to the individual shape

:: (IN OUR WORLD) useful for calendar operations
:: note how powerful this is for regional extraction


+++
((insert slide no.s from new example))
+++
?? cover the LACK-OF multidim interface


[[7]] Features #3 : **Statistics**

:: only one axis at present

We've already seen how to calulate simple statistics (the mean).
  mean = biggus.mean(big_array, axis=i)

This is by analogy with the numpy basic stats (though we don't provide the array methods).

Others follow the same pattern.
:: Though we currently support only the basics = min, max, 

>>>
Implementation:
  * an aggregation is coded into a specific usage framework
  * data is processed in sections known as "chunks", in a series of steps 
    * initialise
    * process chunks
    * finalise

:: Clearly requires independent coding for each operation
:: hence the lack of generality ==> explicit operations, own naming

>>>
The "chunking" is managed by the execution engine.

However, the operation definition can impose constraints.
 * for example, **rank statistics**
:: will require full data on the relevant axis
:: present operations are simpler than this, and don;t impose such constraints


[[8]] Features #4 : **Arithmetic**
For basic arithmetic, simple functions are provided.
:: somewhat limited coverage at present (!!)

>>>
Code example...

So-called "Elementwise" operations are provided.
Somewhat analagous to numpy basic functions (UFUNCS)
   ??? IS THIS APPROPRIATE ??
For example:
((code example))

values = npwrap(np.arange(6.0).reshape(2,3))
constants = npwrap(constant_array((2,3), 100.0))
sum = biggus.add(values, constants)
part_sum = sum[:, 1]
print 'sum:', sum
print 'partial sum:', part_sum
print 'partial sum result:\n', part_sum.ndarray()


:: at present, just add and sub
:: the chunk-aware implementation (a pointwise function) seems obvious
:: unlike statistics, this is not very deep, and **can** be generalised


[[9]] Features #5 : **Parallelism and Concurrency**
((Image))
